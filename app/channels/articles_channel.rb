class ArticlesChannel < ApplicationCable::Channel
  def subscribed
    # Stream name follows "resource_id" pattern (e.g., "post_123", "user_456")
    @stream_name = params[:stream_name]
    reject unless @stream_name

    # TODO: Add stream validation if needed (pattern check, ownership, etc.)
    stream_from @stream_name
  rescue StandardError => e
    handle_channel_error(e)
    reject
  end

  def unsubscribed
    # Any cleanup needed when channel is unsubscribed
  rescue StandardError => e
    handle_channel_error(e)
  end

  # ðŸ“¨ CRITICAL: ALL broadcasts MUST have 'type' field (auto-routes to handleType method)
  #
  # EXAMPLE: Send new message
  # def send_message(data)
  #   message = Message.create!(content: data['content'])
  #
  #   ActionCable.server.broadcast(
  #     @stream_name,
  #     {
  #       type: 'new-message',  # REQUIRED: routes to handleNewMessage() in frontend
  #       id: message.id,
  #       content: message.content,
  #       user_name: message.user.name,
  #       created_at: message.created_at
  #     }
  #   )
  # end

  # EXAMPLE: Send status update
  # def update_status(data)
  #   ActionCable.server.broadcast(
  #     @stream_name,
  #     {
  #       type: 'status-update',  # Routes to handleStatusUpdate() in frontend
  #       status: data['status']
  #     }
  #   )
  # end
  # Generate response with ALL available LLM providers concurrently
  def generate_response(data)
    transcript = data['transcript']
    article_id = data['article_id']
    
    # Create or update article with transcript
    article = if article_id.present?
                Article.find(article_id)
              else
                Article.create!(transcript: transcript)
              end
    
    # List of all available providers
    providers = ['grok', 'qwen', 'deepseek', 'gemini', 'zhipu']
    
    # Trigger jobs for all providers concurrently
    providers.each do |provider|
      llm_config = get_llm_config(provider)
      
      LlmStreamJob.perform_later(
        stream_name: "#{@stream_name}_#{provider}",
        prompt: transcript,
        llm_config: llm_config,
        article_id: article.id,
        provider: provider
      )
    end
    
    # Broadcast article_id back to frontend
    ActionCable.server.broadcast(
      @stream_name,
      {
        type: 'article-created',
        article_id: article.id
      }
    )
  end
  
  # Generate draft by combining transcript + all brainstorm results
  def generate_draft(data)
    article_id = data['article_id']
    selected_model = data['selected_model']
    
    article = Article.find(article_id)
    article.update!(selected_model: selected_model)
    
    # Combine transcript and all brainstorm results
    combined_content = <<~CONTENT
      ã€æˆ‘çš„åŽŸå§‹æƒ³æ³•ã€‘
      #{article.transcript}
      
      ã€Grok çš„è„‘çˆ†ã€‘
      #{article.brainstorm_grok}
      
      ã€Qwen çš„è„‘çˆ†ã€‘
      #{article.brainstorm_qwen}
      
      ã€DeepSeek çš„è„‘çˆ†ã€‘
      #{article.brainstorm_deepseek}
      
      ã€Gemini çš„è„‘çˆ†ã€‘
      #{article.brainstorm_gemini}
      
      ã€æ™ºè°± çš„è„‘çˆ†ã€‘
      #{article.brainstorm_zhipu}
    CONTENT
    
    draft_prompt = <<~PROMPT
      è¯·å°†ä»¥ä¸Šå†…å®¹èžåˆæˆä¸€ç¯‡è¿žè´¯çš„æ–‡ç« è‰ç¨¿ã€‚
      
      è¦æ±‚ï¼š
      1. ä»¥ç¬¬ä¸€äººç§°ï¼ˆä½œè€…è§†è§’ï¼‰å‘ç¬¬ä¸‰äººå¬ä¼—è®²è¿°
      2. èžåˆæˆ‘çš„åŽŸå§‹æƒ³æ³•å’Œ5ä¸ªAIæ¨¡åž‹çš„è„‘çˆ†å†…å®¹
      3. ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œè¯­è¨€æµç•…
      4. å­—æ•°æŽ§åˆ¶åœ¨800-1500å­—
    PROMPT
    
    llm_config = get_llm_config(selected_model)
    
    LlmStreamJob.perform_later(
      stream_name: "#{@stream_name}_draft",
      prompt: combined_content + "\n\n" + draft_prompt,
      llm_config: llm_config,
      article_id: article.id,
      provider: 'draft'
    )
  end
  
  # Generate final article with selected style
  def generate_final(data)
    article_id = data['article_id']
    draft_content = data['draft_content']
    style = data['style']
    
    article = Article.find(article_id)
    article.update!(draft: draft_content, final_style: style)
    
    style_prompt = get_style_prompt(style)
    final_prompt = style_prompt + "\n\nã€è‰ç¨¿å†…å®¹ã€‘\n" + draft_content
    
    llm_config = get_llm_config(article.selected_model)
    
    LlmStreamJob.perform_later(
      stream_name: "#{@stream_name}_final",
      prompt: final_prompt,
      llm_config: llm_config,
      article_id: article.id,
      provider: 'final'
    )
  end

  private
  
  def get_llm_config(provider)
    case provider.to_s
    when 'qwen'
      {
        base_url: ENV.fetch('QWEN_BASE_URL_OPTIONAL'),
        api_key: ENV.fetch('QWEN_API_KEY_OPTIONAL'),
        model: ENV.fetch('QWEN_MODEL_OPTIONAL')
      }
    when 'deepseek'
      {
        base_url: ENV.fetch('DEEPSEEK_BASE_URL_OPTIONAL'),
        api_key: ENV.fetch('DEEPSEEK_API_KEY_OPTIONAL'),
        model: ENV.fetch('DEEPSEEK_MODEL_OPTIONAL')
      }
    when 'gemini'
      {
        base_url: ENV.fetch('GEMINI_BASE_URL_OPTIONAL'),
        api_key: ENV.fetch('GEMINI_API_KEY_OPTIONAL'),
        model: ENV.fetch('GEMINI_MODEL_OPTIONAL')
      }
    when 'zhipu'
      {
        base_url: ENV.fetch('ZHIPU_BASE_URL_OPTIONAL'),
        api_key: ENV.fetch('ZHIPU_API_KEY_OPTIONAL'),
        model: ENV.fetch('ZHIPU_MODEL_OPTIONAL')
      }
    when 'chatgpt'
      {
        base_url: ENV.fetch('CHATGPT_BASE_URL_OPTIONAL'),
        api_key: ENV.fetch('CHATGPT_API_KEY_OPTIONAL'),
        model: ENV.fetch('CHATGPT_MODEL_OPTIONAL')
      }
    when 'grok'
      {
        base_url: ENV.fetch('LLM_BASE_URL'),
        api_key: ENV.fetch('LLM_API_KEY'),
        model: ENV.fetch('LLM_MODEL')
      }
    else
      # Default to Grok
      {
        base_url: ENV.fetch('LLM_BASE_URL'),
        api_key: ENV.fetch('LLM_API_KEY'),
        model: ENV.fetch('LLM_MODEL')
      }
    end
  end

  def get_style_prompt(style)
    case style
    when 'pinker'
      <<~PROMPT
        ä½ æ˜¯è®¤çŸ¥ç§‘å­¦å®¶å²è’‚èŠ¬Â·å¹³å…‹ã€‚è¯·ç”¨ä»–çš„å†™ä½œé£Žæ ¼æ”¹å†™ä»¥ä¸‹å†…å®¹ï¼š
        
        ã€é£Žæ ¼ç‰¹å¾ã€‘
        - ç†æ€§æ¸…æ™°ï¼šç”¨ç§‘å­¦æ€ç»´è§£æž„å¤æ‚æ¦‚å¿µï¼Œå±‚å±‚é€’è¿›è®ºè¯
        - ç±»æ¯”å¤§å¸ˆï¼šå–„ç”¨ç”ŸåŠ¨æ¯”å–»å’Œæ—¥å¸¸åœºæ™¯ç±»æ¯”æŠ½è±¡ç†è®º
        - æ•°æ®æ”¯æ’‘ï¼šå¼•ç”¨ç ”ç©¶ã€å®žéªŒã€ç»Ÿè®¡æ•°æ®å¢žå¼ºè¯´æœåŠ›ï¼ˆå¯è™šæž„åˆç†æ•°æ®ï¼‰
        - ä¼˜é›…å¹½é»˜ï¼šåœ¨ä¸¥è°¨è®ºè¯ä¸­ç©¿æ’æœºæ™ºä¿çš®è¯ï¼Œè®©å­¦æœ¯å˜æœ‰è¶£
        - ç»“æž„æ¸…æ™°ï¼šå…ˆæŠ›é—®é¢˜ï¼Œå†æ‹†è§£åˆ†æžï¼Œæœ€åŽå¾—å‡ºç»“è®º
        
        ã€è¯­è¨€ç‰¹ç‚¹ã€‘
        - é•¿çŸ­å¥äº¤æ›¿ï¼ŒèŠ‚å¥æ„Ÿå¼º
        - ä½¿ç”¨"æƒ³è±¡ä¸€ä¸‹..."ã€"è®©æˆ‘ä»¬çœ‹çœ‹..."ç­‰å¼•å¯¼å¥
        - é¿å…ç©ºæ´žæ¦‚å¿µï¼Œæ¯ä¸ªè§‚ç‚¹éƒ½æœ‰å…·ä½“ä¾‹å­æ”¯æ’‘
        
        è¯·å°†ä»¥ä¸‹è‰ç¨¿æ”¹å†™ä¸ºå²è’‚èŠ¬Â·å¹³å…‹é£Žæ ¼çš„æ–‡ç« ï¼Œä¿æŒç¬¬ä¸€äººç§°è§†è§’ã€‚
      PROMPT
    when 'luozhenyu'
      <<~PROMPT
        ä½ æ˜¯çŸ¥è¯†ä¼ æ’­è€…ç½—æŒ¯å®‡ã€‚è¯·ç”¨ä»–çš„"å¾—åˆ°"é£Žæ ¼æ”¹å†™ä»¥ä¸‹å†…å®¹ï¼š
        
        ã€é£Žæ ¼ç‰¹å¾ã€‘
        - å¼€é—¨è§å±±ï¼šç¬¬ä¸€å¥å°±ç‚¹æ˜Žæ ¸å¿ƒè§‚ç‚¹æˆ–å†²çª
        - æ•…äº‹åŒ–è¡¨è¾¾ï¼šæŠŠæ¦‚å¿µåŒ…è£…æˆæ•…äº‹ï¼Œç”¨åœºæ™¯ä»£æ›¿è¯´æ•™
        - é‡‘å¥é¢‘å‡ºï¼šæ¯æ®µéƒ½æœ‰å¯æ‘˜æŠ„çš„ç²¾ç‚¼è§‚ç‚¹
        - å®žç”¨å¯¼å‘ï¼šå¼ºè°ƒ"å¯¹ä½ æœ‰ä»€ä¹ˆç”¨"ã€"æ€Žä¹ˆç”¨"
        - é™ç»´æ‰“å‡»ï¼šç”¨ç®€å•è¯æ±‡è®²å¤æ‚é“ç†ï¼Œè®©å°å­¦ç”Ÿéƒ½èƒ½å¬æ‡‚
        
        ã€è¯­è¨€ç‰¹ç‚¹ã€‘
        - çŸ­å¥ä¸ºä¸»ï¼ŒèŠ‚å¥ç´§å‡‘
        - å¤§é‡ä½¿ç”¨"ä½ çœ‹"ã€"è¿™å°±æ˜¯"ã€"æ¢å¥è¯è¯´"ç­‰å£è¯­åŒ–è¡”æŽ¥
        - çˆ±ç”¨"ä¸‰æ®µè®º"ç»“æž„ï¼šæ˜¯ä»€ä¹ˆ â†’ ä¸ºä»€ä¹ˆ â†’ æ€Žä¹ˆåŠž
        - ç»“å°¾å‡åŽï¼šä»Žå…·ä½“äº‹ä¾‹ä¸Šå‡åˆ°äººç”Ÿå“²ç†
        
        è¯·å°†ä»¥ä¸‹è‰ç¨¿æ”¹å†™ä¸ºç½—æŒ¯å®‡é£Žæ ¼çš„æ–‡ç« ï¼Œä¿æŒç¬¬ä¸€äººç§°è§†è§’ï¼Œåƒåœ¨è·Ÿæœ‹å‹è®²æ•…äº‹ã€‚
      PROMPT
    when 'wangxiaobo'
      <<~PROMPT
        ä½ æ˜¯ä½œå®¶çŽ‹å°æ³¢ã€‚è¯·ç”¨ä»–çš„æ‚æ–‡é£Žæ ¼æ”¹å†™ä»¥ä¸‹å†…å®¹ï¼š
        
        ã€é£Žæ ¼ç‰¹å¾ã€‘
        - è’è¯žå¹½é»˜ï¼šç”¨åè®½ã€è‡ªå˜²ã€é»‘è‰²å¹½é»˜è§£æž„ä¸¥è‚ƒè¯é¢˜
        - ç†æ€§åå›ï¼šè´¨ç–‘æƒå¨å’Œå¸¸è¯†ï¼Œå±•çŽ°ç‹¬ç«‹æ€è€ƒ
        - ç”Ÿæ´»åŒ–å“²æ€ï¼šä»Žæ—¥å¸¸çäº‹å¼•å‡ºæ·±åˆ»æ´žå¯Ÿ
        - çœŸè¯šç›´ç™½ï¼šä¸è£…è…”ä½œåŠ¿ï¼Œç”¨å¤§ç™½è¯è¯´çœŸå¿ƒè¯
        - è·³è·ƒæ€ç»´ï¼šçœ‹ä¼¼é—²èŠï¼Œå®žåˆ™æš—è—é€»è¾‘çº¿
        
        ã€è¯­è¨€ç‰¹ç‚¹ã€‘
        - å¤§é‡ä½¿ç”¨"æˆ‘è§‰å¾—"ã€"è¯´å®žè¯"ã€"æœ‰æ„æ€çš„æ˜¯"
        - çˆ±ä¸¾è’è¯žä¾‹å­å¯¹æ¯”ï¼ˆ"å°±åƒ..."ï¼‰
        - çªç„¶æ’å…¥ä¸ªäººç»åŽ†æˆ–å‡è®¾åœºæ™¯
        - å¥å¼éšæ„ï¼Œåƒè·Ÿè¯»è€…èŠå¤©
        - ç»“å°¾å¾€å¾€å‡ºäººæ„æ–™ï¼Œç•™ä¸‹å›žå‘³
        
        è¯·å°†ä»¥ä¸‹è‰ç¨¿æ”¹å†™ä¸ºçŽ‹å°æ³¢é£Žæ ¼çš„æ–‡ç« ï¼Œä¿æŒç¬¬ä¸€äººç§°è§†è§’ï¼Œå¸¦ç€çŽ©ä¸–ä¸æ­çš„æ™ºæ…§ã€‚
      PROMPT
    else
      ""
    end
  end
  
  # def current_user
  #   @current_user ||= connection.current_user
  # end
end
